Kover Learning Report
=====================

Running time: 0:28:04.869675

Configuration:
--------------
authorized_rules: 
bound_delta: 0.05
bound_max_genome_size: 10461658
class_importance: ['0.25', '0.5', '0.75', '1.0']
criterion: ['gini']
dataset: /scratch/adrouin/patric_data/single_species_datasets/nicotinamide___260162261243005161/dataset.kover
hp_choice: cv
max_depth: [20]
min_samples_split: [2]
n_cpu: 2
output_dir: /scratch/adrouin/patric_experiments/single_species_datasets/cart_cv/nicotinamide___260162261243005161/train_0.800_seed_6_10_folds
progress: False
split: train_0.800_seed_6_10_folds
verbose: False

Data summary:
-------------
Dataset file: /scratch/adrouin/patric_data/single_species_datasets/nicotinamide___260162261243005161/dataset.kover
Dataset UUID: 2407f734-0767-11e8-a171-001b2193b764
Phenotype: Nicotinamide___Mycobacterium_Tuberculosis___Metadata_Date_2018-01-29
Split: train_0.800_seed_6_10_folds
Number of genomes used for training: 134 (Group sensitive: 66, Group resistant: 68)
Number of genomes used for testing: 33 (Group sensitive: 17, Group resistant: 16)

Hyperparameter Values:
----------------------
Selection strategy: 10-fold cross-validation (score = 0.16374)
Criterion: gini
Class importance: class sensitive: 0.500, class resistant: 0.250
Maximum tree depth: 20
Minimum samples to split a node (examples): 2.000
Pruning alpha: 0.01936492

Metrics (training data)
-----------------------
Error Rate: 0.13433
Sensitivity: 0.76471
Specificity: 0.9697
Precision: 0.96296
Recall: 0.76471
F1 Score: 0.85246
True Positives: 52.0
True Negatives: 64.0
False Positives: 2.0
False Negatives: 16.0

Metrics (testing data)
----------------------
Error Rate: 0.15152
Sensitivity: 0.8125
Specificity: 0.88235
Precision: 0.86667
Recall: 0.8125
F1 Score: 0.83871
True Positives: 13.0
True Negatives: 15.0
False Positives: 2.0
False Negatives: 3.0

Model (2 rules, depth = 2):

    resistant
   /
Presence(ACCACCCGCACGGCGACGCGTCGATCTACGA)
   \
        resistant
       /
    Presence(ACCAGGCGCGGCCCAGGTCCGCGTGCTGCCG)
       \
        sensitive

